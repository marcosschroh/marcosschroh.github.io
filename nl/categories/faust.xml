<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Marcos Schroh (Berichten over faust)</title><link>https://marcosschroh.github.io/</link><description></description><atom:link href="https://marcosschroh.github.io/nl/categories/faust.xml" rel="self" type="application/rss+xml"></atom:link><language>nl</language><copyright>Contents © 2019 &lt;a href="mailto:schrohm@gmail.com"&gt;Marcos Schroh&lt;/a&gt; </copyright><lastBuildDate>Sat, 28 Sep 2019 15:33:59 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Python Schema Registry Client</title><link>https://marcosschroh.github.io/nl/posts/nl/schema-registry-client/</link><dc:creator>Marcos Schroh</dc:creator><description>&lt;div&gt;&lt;p&gt;Businesses collect large amounts of data, and data can be analized in real time. Usually, we use &lt;a href="https://kafka.apache.org/"&gt;Kafka&lt;/a&gt; and a framework such us &lt;a href="https://flink.apache.org/"&gt;Flink&lt;/a&gt; or &lt;a href="https://faust.readthedocs.io/en/latest/"&gt;Faust&lt;/a&gt; to proccess data, but we do not include a way to validate it. 
This is why &lt;a href="https://thrift.apache.org/"&gt;Thrift&lt;/a&gt;, &lt;a href="https://developers.google.com/protocol-buffers"&gt;Protocol Buffers&lt;/a&gt; and &lt;a href="https://avro.apache.org/docs/current/"&gt;Apache Avro&lt;/a&gt; were developed. In this post I want to talk about &lt;code&gt;Avro Schemas&lt;/code&gt; and how to integrate them with &lt;code&gt;Faust&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;Apache Avro and Avro Schemas?&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;Avro&lt;/code&gt; is a row-oriented remote procedure call and data serialization framework developed within Apache's Hadoop project. It uses JSON for defining data types and protocols, and serializes data in a compact binary format. Its primary use is in Apache Hadoop, where it can provide both a serialization format for persistent data, and a wire format for communication between Hadoop nodes, and from client programs to the Hadoop services. Avro uses a schema (avro schema) to structure the data that is being encoded. It has two different types of schema languages; one for human editing (Avro IDL) and another which is more machine-readable based on (JSON).&lt;/p&gt;
&lt;p&gt;It is similar to Thrift and Protocol Buffers, but does not require running a code-generation program when a schema changes (unless desired for statically-typed languages). &lt;/p&gt;
&lt;p&gt;Avro Schema example:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
   &lt;span class="nt"&gt;"namespace"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"example.avro"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
   &lt;span class="nt"&gt;"type"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"record"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
   &lt;span class="nt"&gt;"name"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"User"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
   &lt;span class="nt"&gt;"fields"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
      &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nt"&gt;"name"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"name"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;"type"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"string"&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt;
      &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nt"&gt;"name"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"favorite_number"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="nt"&gt;"type"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"int"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"null"&lt;/span&gt;&lt;span class="p"&gt;]},&lt;/span&gt;
      &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nt"&gt;"name"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"favorite_color"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;"type"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"string"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"null"&lt;/span&gt;&lt;span class="p"&gt;]}&lt;/span&gt;
   &lt;span class="p"&gt;]&lt;/span&gt; 
 &lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;Now that we know &lt;code&gt;avro schemas&lt;/code&gt; are, we should talk about where they are stored. We need a place where producers and cosumers can get them, and this is why a &lt;code&gt;Schema Registry&lt;/code&gt; exist. The &lt;a href="https://docs.confluent.io/current/schema-registry/index.html"&gt;Confluent Schema Registry&lt;/a&gt;is a schema management that taht provides a RESTful interface for storing, serving and versioning schemas. &lt;/p&gt;
&lt;h3&gt;Scenario&lt;/h3&gt;
&lt;p&gt;We could think about a producer that uses a schema to serialize data and compact it into a binary representation and a consumer that deserialize the binary to get the original data cheking it with the corresponding schema.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Confluent Architecture" src="https://marcosschroh.github.io/data-streaming/confluent_architecture.png"&gt;&lt;/p&gt;
&lt;p&gt;The producer and consumer have to serialize/deserialize messages using the Schema Registry every time that they send/receive events to/from &lt;code&gt;Kafka topics&lt;/code&gt;. We can imagine the producer and consumer as &lt;code&gt;Faust&lt;/code&gt; application that are able to interact with the Schema Registry Server. In order to achive this, I have created &lt;a href="https://github.com/marcosschroh/python-schema-registry-client/"&gt;Python Schema Registry Client&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A Python Rest Client to interact against schema-registry confluent server to manage Avro Schemas resources.
Also, has a &lt;code&gt;MessageSerializer&lt;/code&gt; in order to serialize/deserialize events using avro schemas. &lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;Faust Integration&lt;/h3&gt;
&lt;p&gt;Asumming that you know &lt;code&gt;Faust&lt;/code&gt;, we need to define a custom codec and a custom serializer to be able to talk with the Schema Registry, and to do that, we will use the &lt;code&gt;MessageSerializer&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;For the demonstration, let's imagine that we have the following schema:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="s2"&gt;"type"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"record"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;"namespace"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"com.example"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;"name"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"AvroUsers"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;"fields"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
        &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;"name"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"first_name"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"type"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"string"&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt;
        &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;"name"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"last_name"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"type"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"string"&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;Let's register the custom codec:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# codecs.codec.py&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;schema_registry.client&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;SchemaRegistryClient&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;schema&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;schema_registry.serializers&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;FaustSerializer&lt;/span&gt;

&lt;span class="c1"&gt;# create an instance of the `SchemaRegistryClient`&lt;/span&gt;
&lt;span class="n"&gt;client&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;SchemaRegistryClient&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;settings&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;SCHEMA_REGISTRY_URL&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# schema that we want to use. For this example we &lt;/span&gt;
&lt;span class="c1"&gt;# are using a dict, but this schema could be located in a file called avro_user_schema.avsc&lt;/span&gt;
&lt;span class="n"&gt;avro_user_schema&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;schema&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AvroSchema&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;
     &lt;span class="s2"&gt;"type"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"record"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
     &lt;span class="s2"&gt;"namespace"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"com.example"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
     &lt;span class="s2"&gt;"name"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"AvroUsers"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
     &lt;span class="s2"&gt;"fields"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
       &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;"name"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"first_name"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"type"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"string"&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt;
       &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;"name"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"last_name"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"type"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"string"&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
     &lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;})&lt;/span&gt;

&lt;span class="n"&gt;avro_user_serializer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;FaustSerializer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;client&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"users"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;avro_user_schema&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="c1"&gt;# function used to register the codec&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;avro_user_codec&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;avro_user_serializer&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;and in setup.py the following code in order to tell faust where to find the custom codecs.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# setup.py&lt;/span&gt;

&lt;span class="n"&gt;setup&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="o"&gt;...&lt;/span&gt;
    &lt;span class="n"&gt;entry_points&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="s1"&gt;'console_scripts'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
            &lt;span class="s1"&gt;'example = example.app:main'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="s1"&gt;'faust.codecs'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
            &lt;span class="s1"&gt;'avro_users = example.codecs.avro:avro_user_codec'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="p"&gt;},&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;Now the final step is to integrate the faust model with the AvroSerializer:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# users.models&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;UserModel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;faust&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Record&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;serializer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'avro_users'&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;first_name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;
    &lt;span class="n"&gt;last_name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;
&lt;/pre&gt;


&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# users.agents.py&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;logging&lt;/span&gt;

&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;your_project.app&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;app&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;.codecs.codec&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;avro_user_serializer&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;.models&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;UserModel&lt;/span&gt;

&lt;span class="n"&gt;users_topic&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;app&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;topic&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'avro_users'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;partitions&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;value_type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;UserModel&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;logger&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getLogger&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="vm"&gt;__name__&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="nd"&gt;@app.agent&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;users_topic&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;async&lt;/span&gt; &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;users&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;users&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;async&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;user&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;users&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;logger&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;info&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Event received in topic avro_users"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;logger&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;info&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"First Name: {user.first_name}, last name {user.last_name}"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="nd"&gt;@app.timer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;5.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;on_leader&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;async&lt;/span&gt; &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;publish_users&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;logger&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;info&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'PUBLISHING ON LEADER FOR USERS APP!'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;user&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;"first_name"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"foo"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"last_name"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"bar"&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="n"&gt;await&lt;/span&gt; &lt;span class="n"&gt;users&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;send&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;user&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;value_serializer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;avro_user_serializer&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;Now our application is able to send and receive message using arvo schemas!!!! :-) You can take a look the code example &lt;a href="https://github.com/marcosschroh/faust-docker-compose-example/blob/master/faust-project/example/codecs/avro.py"&gt;here&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><guid>https://marcosschroh.github.io/nl/posts/nl/schema-registry-client/</guid><pubDate>Tue, 24 Sep 2019 17:32:55 GMT</pubDate></item><item><title>Cookiecutter Faust</title><link>https://marcosschroh.github.io/nl/posts/nl/cookiecutter-faust/</link><dc:creator>Marcos Schroh</dc:creator><description>&lt;div&gt;&lt;p&gt;Bedrijven krijgen long som gegevens, en gegevens specialists kunnen extract informatie en leren uit het. Wants we kunnen leverenen gegevens in real tijd, applicatie &lt;code&gt;Data Streaming&lt;/code&gt; en &lt;code&gt;Processing Tecnics&lt;/code&gt;, kennis kun ogenblikkelijk ontdekt.&lt;/p&gt;
&lt;p&gt;Frameworks zoals &lt;a href="https://kafka.apache.org/documentation/streams/"&gt;Kafka Streams&lt;/a&gt;, &lt;a href="https://spark.apache.org/"&gt;Apache Spark&lt;/a&gt;, en &lt;a href="https://flink.apache.org/"&gt;Flink&lt;/a&gt; zijn gebruiken voor dit wit, maar special ondersteuning voor &lt;code&gt;Java&lt;/code&gt; en &lt;code&gt;Scala&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Kort geleden, een nieuwe framework hebt geboren voor Python: &lt;a href="https://faust.readthedocs.io/en/latest/"&gt;Faust&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Faust is a stream processing library, porting the ideas from Kafka Streams to Python.
It is used to build high performance distributed systems and real-time data pipelines that process billions of events every day.
Faust provides both stream processing and event processing, sharing similarity with tools such as Kafka Streams, Apache Spark/Storm/Samza/Flink,
It does not use a DSL, it’s just Python! This means you can use all your favorite Python libraries when stream processing: NumPy, PyTorch, Pandas, NLTK, Django, Flask, SQLAlchemy, ++
Faust requires Python 3.6 or later for the new async/await syntax, and variable type annotations.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Als we bruiken een gegevens streaming technologie, we hebben en broker stellen nodig, bij voorbeeld&lt;a href="https://kafka.apache.org/"&gt;kafka&lt;/a&gt;, en &lt;code&gt;kafka&lt;/code&gt; hebt &lt;a href="https://zookeeper.apache.org/"&gt;zookeeper&lt;/a&gt; nodig, en this is when we start struggling a bit because the different parts have to be installed and configured in order to play together. Indeed, we can go further, and use services like &lt;code&gt;Schema Registry&lt;/code&gt; and &lt;code&gt;Rocks DB&lt;/code&gt; to make more robust our stack, and again, we need to spend time configuring them.  &lt;/p&gt;
&lt;p&gt;Dus, Ik heb gecreëerd en kleine project heet &lt;a href="https://github.com/marcosschroh/cookiecutter-faust"&gt;cookiecutter-faust&lt;/a&gt;: &lt;code&gt;A Cookiecutter template for creating Faust projects quickly&lt;/code&gt;, means that all the necessary services are pre-configured and the project skeleton is generated for you.&lt;/p&gt;
&lt;p&gt;The requirements zijn &lt;code&gt;cookiecutter&lt;/code&gt;, &lt;code&gt;Docker&lt;/code&gt; en &lt;code&gt;Docker Compose&lt;/code&gt;.&lt;/p&gt;
&lt;h4&gt;Kenmerken&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Faust 1.5.4&lt;/li&gt;
&lt;li&gt;Python 3.6 and 3.7&lt;/li&gt;
&lt;li&gt;Docker en docker-compose ondersteuning&lt;/li&gt;
&lt;li&gt;Useful commands included in Makefile&lt;/li&gt;
&lt;li&gt;Project skeleton is defined as a medium/large project according to faust layout&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;setup.py&lt;/code&gt; has the entrypoint in order to solve the entrypoint problem in Faust&lt;/li&gt;
&lt;li&gt;Include a &lt;code&gt;settings.py&lt;/code&gt; as &lt;code&gt;Django&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Include an App example with tests&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Gebruiken&lt;/h4&gt;
&lt;p&gt;Is super makkelijk. Eerste, we hebben install &lt;code&gt;coockicutter&lt;/code&gt; nodig. &lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;pip install &lt;span class="s2"&gt;"cookiecutter&amp;gt;=1.4.0"&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;Then, just run:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;cookiecutter https://github.com/marcosschroh/cookiecutter-faust
&lt;/pre&gt;


&lt;p&gt;antword de vragens met jouw opties:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;project_name &lt;span class="o"&gt;[&lt;/span&gt;My Awesome Faust Project&lt;span class="o"&gt;]&lt;/span&gt;: super faust
project_slug &lt;span class="o"&gt;[&lt;/span&gt;super_faust&lt;span class="o"&gt;]&lt;/span&gt;:
description &lt;span class="o"&gt;[&lt;/span&gt;My Awesome Faust Project!&lt;span class="o"&gt;]&lt;/span&gt;:
long_description &lt;span class="o"&gt;[&lt;/span&gt;My Awesome Faust Project!&lt;span class="o"&gt;]&lt;/span&gt;:
author_name &lt;span class="o"&gt;[&lt;/span&gt;Marcos Schroh&lt;span class="o"&gt;]&lt;/span&gt;:
author_email &lt;span class="o"&gt;[&lt;/span&gt;marcos-schroh@gmail.com&lt;span class="o"&gt;]&lt;/span&gt;:
version &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;.1.0&lt;span class="o"&gt;]&lt;/span&gt;:
Select open_source_license:
&lt;span class="m"&gt;1&lt;/span&gt; - MIT
&lt;span class="m"&gt;2&lt;/span&gt; - BSD
&lt;span class="m"&gt;3&lt;/span&gt; - GPLv3
&lt;span class="m"&gt;4&lt;/span&gt; - Apache Software License &lt;span class="m"&gt;2&lt;/span&gt;.0
&lt;span class="m"&gt;5&lt;/span&gt; - Not open &lt;span class="nb"&gt;source&lt;/span&gt;
Choose from &lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;2&lt;/span&gt;, &lt;span class="m"&gt;3&lt;/span&gt;, &lt;span class="m"&gt;4&lt;/span&gt;, &lt;span class="m"&gt;5&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;2&lt;/span&gt;, &lt;span class="m"&gt;3&lt;/span&gt;, &lt;span class="m"&gt;4&lt;/span&gt;, &lt;span class="m"&gt;5&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;:
use_pycharm &lt;span class="o"&gt;[&lt;/span&gt;n&lt;span class="o"&gt;]&lt;/span&gt;:
use_docker &lt;span class="o"&gt;[&lt;/span&gt;n&lt;span class="o"&gt;]&lt;/span&gt;: y
include_docker_compose &lt;span class="o"&gt;[&lt;/span&gt;n&lt;span class="o"&gt;]&lt;/span&gt;: y
include_page_view_tutorial &lt;span class="o"&gt;[&lt;/span&gt;n&lt;span class="o"&gt;]&lt;/span&gt;: y
worker_port &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="m"&gt;6066&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;:
kafka_server_environment_variable &lt;span class="o"&gt;[&lt;/span&gt;KAFKA_BOOTSTRAP_SERVER&lt;span class="o"&gt;]&lt;/span&gt;:
include_codec_example &lt;span class="o"&gt;[&lt;/span&gt;y&lt;span class="o"&gt;]&lt;/span&gt;:
Select faust_loglevel:
&lt;span class="m"&gt;1&lt;/span&gt; - CRITICAL
&lt;span class="m"&gt;2&lt;/span&gt; - ERROR
&lt;span class="m"&gt;3&lt;/span&gt; - WARNING
&lt;span class="m"&gt;4&lt;/span&gt; - INFO
&lt;span class="m"&gt;5&lt;/span&gt; - DEBUG
&lt;span class="m"&gt;6&lt;/span&gt; - NOTSET
Choose from &lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;2&lt;/span&gt;, &lt;span class="m"&gt;3&lt;/span&gt;, &lt;span class="m"&gt;4&lt;/span&gt;, &lt;span class="m"&gt;5&lt;/span&gt;, &lt;span class="m"&gt;6&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;2&lt;/span&gt;, &lt;span class="m"&gt;3&lt;/span&gt;, &lt;span class="m"&gt;4&lt;/span&gt;, &lt;span class="m"&gt;5&lt;/span&gt;, &lt;span class="m"&gt;6&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;: &lt;span class="m"&gt;4&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;En nu je bent klaar te coding jouw Data Streaming applicatie zonder spending tijd in configuratie!&lt;/p&gt;
&lt;p&gt;De projectdocumentatie is &lt;a href="https://github.com/marcosschroh/cookiecutter-faust/blob/master/README.md"&gt;hier&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><category>data streaming</category><category>faust</category><category>python</category><guid>https://marcosschroh.github.io/nl/posts/nl/cookiecutter-faust/</guid><pubDate>Fri, 07 Jun 2019 10:40:51 GMT</pubDate></item></channel></rss>